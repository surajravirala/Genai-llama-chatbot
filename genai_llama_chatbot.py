# -*- coding: utf-8 -*-
"""Genai_llama_chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UWRftgHzQhlxWjqoP991fJcwRMgekP6H
"""

!pip install transformers langchain torch gradio

# Install the classic package
!pip install langchain_classic

import torch
from transformers import AutoModelForCausalLM,AutoTokenizer
from langchain_classic.memory import ConversationBufferWindowMemory
import gradio as gr

model_name="meta-llama/Llama-2-7b-chat-hf"
token="hf_GXBSmMplmesJ####FwzeuEcD#"

device=torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"using device: {device}")

model=AutoModelForCausalLM.from_pretrained(model_name, token=token).to(device)
tokenizer=AutoTokenizer.from_pretrained(model_name, token=token)

def chatbot_responce(user_input):

  inputs=tokenizer(user_input,return_tensors="pt",max_length=512,truncation=True).to(device)


  outputs=model.generate(**inputs)


  bot_reply=tokenizer.decode(outputs[0],skip_special_tokens=True)


  if bot_reply.startswith(user_input):
      bot_reply=bot_reply[len(user_input):].strip()
  return bot_reply

def chatbot_interface(user_input,history):

  response=chatbot_responce(user_input)

  history.append((user_input,response))

  return history,""


demo=gr.Blocks()

with demo:
  gr.Markdown("## GENAI Chatbot")
  chatbot=gr.Chatbot()
  with gr.Row():
    user_input=gr.Textbox(placeholder="Type your message here",lines=1)
    send_button=gr.Button("Send")

  history=gr.State([])

  send_button.click(chatbot_interface,inputs=[user_input,history],outputs=[chatbot, user_input])
  user_input.submit(chatbot_interface,inputs=[user_input,history],outputs=[chatbot, user_input])

demo.launch(share=True)

from huggingface_hub import login

# Ensure the token variable is available (it should be from previous cells)
if 'token' in locals() or 'token' in globals():
    login(token=token)
    print("Successfully logged in to Hugging Face.")
else:
    print("Hugging Face token not found. Please ensure it's defined.")

from huggingface_hub import HfApi

try:
    api = HfApi(token=token)
    user_info = api.whoami()
    print(f"Successfully authenticated as: {user_info['name']}")
    # Further check for model access if needed, e.g., by trying to fetch model info
    model_info = api.model_info(model_name)
    print(f"Successfully retrieved info for model: {model_info.modelId}")
    print("Your Hugging Face token is valid and has access to the specified model.")
except Exception as e:
    print(f"Error during Hugging Face token verification or model access check: {e}")
    print("Please ensure your token is correct, has the necessary permissions, and you have accepted the model's terms on Hugging Face.")

!pip install nbstripout
!nbstripout your_notebook_name.ipynb